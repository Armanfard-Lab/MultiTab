{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_lightning import seed_everything\n",
    "from config import create_config, create_data_loaders\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "args = {\n",
    "    'model': 'mtt', # dummy model name\n",
    "    'dataset': 'higgs', # name of the dataset\n",
    "    'data_root': '/path/to/data/', # root directory of the data\n",
    "    'run_name': 'auto',\n",
    "    'cuda': 1,\n",
    "    'seed': 1\n",
    "}\n",
    "\n",
    "# Load configuration from YAML file\n",
    "config = create_config(args)\n",
    "\n",
    "# Set seeds\n",
    "seed_everything(config['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb    \n",
    "\n",
    "# Load the dataset\n",
    "if config['dataset'] == 'aliexpress':\n",
    "    dataset = create_data_loaders(config, return_splits=True)\n",
    "    x_train = np.concatenate((dataset['train'].x_cat, dataset['train'].x_num), axis=1)\n",
    "    y_train = np.concatenate((dataset['train'].y_click.unsqueeze(-1), dataset['train'].y_conversion.unsqueeze(-1)), axis=1)\n",
    "    x_test = np.concatenate((dataset['test'].x_cat, dataset['test'].x_num), axis=1)\n",
    "    y_test = np.concatenate((dataset['test'].y_click.unsqueeze(-1), dataset['test'].y_conversion.unsqueeze(-1)), axis=1)\n",
    "\n",
    "elif config['dataset'] == 'acs_income' or config['dataset'] == 'higgs':\n",
    "    dataset = create_data_loaders(config, return_splits=True)\n",
    "    x_train = dataset['train'].features.numpy()\n",
    "    y_train = np.concatenate(([target.unsqueeze(-1).numpy() for target in dataset['train'].targets.values()]), axis=1)\n",
    "    x_test = dataset['test'].features.numpy()\n",
    "    y_test = np.concatenate(([target.unsqueeze(-1).numpy() for target in dataset['test'].targets.values()]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'format': 'h5',\n",
       " 'name': 'higgs',\n",
       " 'num_features': 21,\n",
       " 'path': '/higgs11M/',\n",
       " 'short_name': 'hig',\n",
       " 'task_out_dim': {'Target': 1,\n",
       "  'm_bb': 1,\n",
       "  'm_jj': 1,\n",
       "  'm_jjj': 1,\n",
       "  'm_jlv': 1,\n",
       "  'm_lv': 1,\n",
       "  'm_wbb': 1,\n",
       "  'm_wwbb': 1},\n",
       " 'task_type': {'Target': 'binary',\n",
       "  'm_bb': 'regression',\n",
       "  'm_jj': 'regression',\n",
       "  'm_jjj': 'regression',\n",
       "  'm_jlv': 'regression',\n",
       "  'm_lv': 'regression',\n",
       "  'm_wbb': 'regression',\n",
       "  'm_wwbb': 'regression'},\n",
       " 'tasks': ['Target',\n",
       "  'm_jj',\n",
       "  'm_jjj',\n",
       "  'm_lv',\n",
       "  'm_jlv',\n",
       "  'm_bb',\n",
       "  'm_wbb',\n",
       "  'm_wwbb'],\n",
       " 'seperate_ft_types': False,\n",
       " 'feature_dims': {'0': 3,\n",
       "  '1': 3,\n",
       "  '2': 3,\n",
       "  '3': 3,\n",
       "  '4': 1,\n",
       "  '5': 1,\n",
       "  '6': 1,\n",
       "  '7': 1,\n",
       "  '8': 1,\n",
       "  '9': 1,\n",
       "  '10': 1,\n",
       "  '11': 1,\n",
       "  '12': 1,\n",
       "  '13': 1,\n",
       "  '14': 1,\n",
       "  '15': 1,\n",
       "  '16': 1,\n",
       "  '17': 1,\n",
       "  '18': 1,\n",
       "  '19': 1,\n",
       "  '20': 1}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (9900000, 21)\n",
      "y_train shape: (9900000, 8)\n",
      "x_test shape: (550000, 21)\n",
      "y_test shape: (550000, 8)\n"
     ]
    }
   ],
   "source": [
    "# check all the shapes\n",
    "print(f\"x_train shape: {x_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AliExpresss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the training and testing data into DMatrix format (XGBoost specific data structure)\n",
    "dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "dtest = xgb.DMatrix(x_test, label=y_test)\n",
    "\n",
    "# Set parameters for XGBoost\n",
    "params = {\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.2,\n",
    "    'objective': 'binary:logistic',  # binary objective\n",
    "    'eval_metric': 'auc',  # Evaluation metric\n",
    "}\n",
    "\n",
    "# Train the XGBoost model\n",
    "num_rounds = 200  # Number of training rounds\n",
    "bst = xgb.train(params, dtrain, num_rounds)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = bst.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: click, LogLoss: 0.10830, AUC-ROC: 0.72272\n",
      "Task: conversion, LogLoss: 0.00626, AUC-ROC: 0.84519\n"
     ]
    }
   ],
   "source": [
    "from torchmetrics.classification import AUROC\n",
    "\n",
    "# Initialize the BinaryAUROC metric\n",
    "auroc = AUROC(task='binary')\n",
    "bce = torch.nn.BCELoss()\n",
    "\n",
    "# Compute AUROC\n",
    "for i, task in enumerate(config.data.tasks):\n",
    "    auc_roc_score = auroc(torch.tensor(y_pred[:,i]), torch.tensor(y_test[:,i]))\n",
    "    bce_loss = bce(torch.tensor(y_pred[:,i]), torch.tensor(y_test[:,i]).to(torch.float32))\n",
    "    print(f\"Task: {task}, LogLoss: {bce_loss:.5f}, AUC-ROC: {auc_roc_score:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACS Income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the training and testing data into DMatrix format (XGBoost specific data structure)\n",
    "dtrain_binary = xgb.DMatrix(x_train, label=y_train[:,0])\n",
    "dtest_binary = xgb.DMatrix(x_test, label=y_test[:,0])\n",
    "\n",
    "params_binary = {\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.2,\n",
    "    'objective': 'binary:logistic',  # binary objective\n",
    "    'eval_metric': 'auc',  # Evaluation metric\n",
    "}\n",
    "\n",
    "dtrain_multi = xgb.DMatrix(x_train, label=y_train[:,1])\n",
    "dtest_multi = xgb.DMatrix(x_test, label=y_test[:,1])\n",
    "\n",
    "params_multiclass = {\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.1,\n",
    "    'objective': 'multi:softprob',  # multi-class objective\n",
    "    'num_class': 5,  # Number of classes for multi-class classification\n",
    "    'eval_metric': 'auc',  # Evaluation metric\n",
    "}\n",
    "\n",
    "# Train the XGBoost model\n",
    "num_rounds = 200  # Number of training rounds\n",
    "bst_binary = xgb.train(params_binary, dtrain_binary, num_rounds)\n",
    "bst_multi = xgb.train(params_multiclass, dtrain_multi, num_rounds)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred_binary = bst_binary.predict(dtest_binary)\n",
    "y_pred_multi = bst_multi.predict(dtest_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: PINCP, AUC-ROC: 0.89738\n",
      "Task: MAR, AUC-ROC: 0.88438\n"
     ]
    }
   ],
   "source": [
    "from torchmetrics.classification import AUROC\n",
    "\n",
    "# Initialize the BinaryAUROC metric\n",
    "auroc = AUROC(task='binary')\n",
    "\n",
    "# Initialize the Multclass AUROC metric\n",
    "auroc_multiclass = AUROC(task='multiclass', num_classes=5)\n",
    "\n",
    "auc_roc_score = auroc(torch.tensor(y_pred_binary), torch.tensor(y_test[:, 0]))\n",
    "print(f\"Task: {config.data.tasks[0]}, AUC-ROC: {auc_roc_score:.5f}\")\n",
    "\n",
    "auc_roc_score = auroc_multiclass(torch.tensor(y_pred_multi), torch.tensor(y_test[:, 1]).long())\n",
    "print(f\"Task: {config.data.tasks[1]}, AUC-ROC: {auc_roc_score:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Higgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the training and testing data into DMatrix format (XGBoost specific data structure)\n",
    "dtrain_binary = xgb.DMatrix(x_train, label=y_train[:,0])\n",
    "dtest_binary = xgb.DMatrix(x_test, label=y_test[:,0])\n",
    "\n",
    "# Set parameters for XGBoost\n",
    "params_binary = {\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.2,\n",
    "    'objective': 'binary:logistic',  # binary objective\n",
    "    'eval_metric': 'auc',  # Evaluation metric\n",
    "}\n",
    "\n",
    "dtrain_regression = xgb.DMatrix(x_train, label=y_train[:,1:])\n",
    "dtest_regression = xgb.DMatrix(x_test, label=y_test[:,1:])\n",
    "\n",
    "# config for XGBOOST on regression\n",
    "params = {\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.1,\n",
    "    'objective': 'reg:squarederror',  # regression objective\n",
    "    'eval_metric': 'rmse',  # Evaluation metric\n",
    "}\n",
    "\n",
    "# Train the XGBoost model\n",
    "num_rounds = 100  # Number of training rounds\n",
    "bst_binary = xgb.train(params_binary, dtrain_binary, num_rounds)\n",
    "bst_regression = xgb.train(params, dtrain_regression, num_rounds)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred_binary = bst_binary.predict(dtest_binary)\n",
    "y_pred_regression = bst_regression.predict(dtest_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: Target, AUC-ROC: 0.71872\n",
      "Task: m_jj, Explained Variance: 0.24298\n",
      "Task: m_jjj, Explained Variance: 0.29085\n",
      "Task: m_lv, Explained Variance: 0.22060\n",
      "Task: m_jlv, Explained Variance: 0.40948\n",
      "Task: m_bb, Explained Variance: 0.77195\n",
      "Task: m_wbb, Explained Variance: 0.59887\n",
      "Task: m_wwbb, Explained Variance: 0.54457\n"
     ]
    }
   ],
   "source": [
    "from torchmetrics.classification import AUROC\n",
    "from torchmetrics.regression import ExplainedVariance\n",
    "\n",
    "# Initialize the metric\n",
    "auroc = AUROC(task='binary')\n",
    "explained_variance = ExplainedVariance()\n",
    "\n",
    "auc_roc_score = auroc(torch.tensor(y_pred_binary), torch.tensor(y_test[:, 0]))\n",
    "print(f\"Task: {config.data.tasks[0]}, AUC-ROC: {auc_roc_score:.5f}\")\n",
    "\n",
    "for i, task in enumerate(config.data.tasks[1:]):\n",
    "    explained_variance_score = explained_variance(torch.tensor(y_pred_regression[:, i]), torch.tensor(y_test[:, i + 1]))\n",
    "    print(f\"Task: {task}, Explained Variance: {explained_variance_score:.5f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mtt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
