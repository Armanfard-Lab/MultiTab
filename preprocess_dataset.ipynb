{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb1d2627",
   "metadata": {},
   "source": [
    "# Dataset Preprocessing for H5 Format\n",
    "\n",
    "This notebook allows you to:\n",
    "1. Load datasets from filesystem (CSV, parquet, etc.) or OpenML\n",
    "2. Select and configure target variables (including additional targets)\n",
    "3. Preprocess features (categorical/continuous separation)\n",
    "4. Save in H5 format compatible with the modular dataset system\n",
    "5. Validate the created H5 dataset\n",
    "\n",
    "## Requirements\n",
    "- pandas\n",
    "- numpy\n",
    "- h5py\n",
    "- scikit-learn\n",
    "- openml (for OpenML datasets)\n",
    "- dataset_utils (our custom module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0d7bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import logging\n",
    "\n",
    "# Data science imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our custom utilities\n",
    "from data.dataset_utils import create_h5_from_dataframe, validate_h5_dataset, inspect_h5_dataset\n",
    "\n",
    "# Optional: Import OpenML if available\n",
    "try:\n",
    "    import openml\n",
    "    OPENML_AVAILABLE = True\n",
    "    print(\"OpenML available\")\n",
    "except ImportError:\n",
    "    OPENML_AVAILABLE = False\n",
    "    print(\"OpenML not available. Install with: pip install openml\")\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a1fc11",
   "metadata": {},
   "source": [
    "## Step 0: Configuration\n",
    "\n",
    "Set up the basic configuration for your dataset preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6a7c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for Higgs dataset\n",
    "CONFIG = {\n",
    "    'dataset_name': 'higgs',  # Name for your dataset\n",
    "    'short_name': 'hig',            # Short name for the dataset\n",
    "    'output_dir': '/path/to/data',   # Where to save the H5 file (must match DATA_ROOT in run.sh)\n",
    "    'data_source': 'openml',           # 'file' or 'openml'\n",
    "    'file_path': None,                  # Path to your data file (if using 'file')\n",
    "    'openml_id': 45570,                  # OpenML dataset ID (if using 'openml')\n",
    "    'train_split': 0.90,\n",
    "    'val_split': 0.05,\n",
    "    'test_split': 0.05,\n",
    "    'random_seed': 42,\n",
    "    'categorical_threshold': 4,        # Max unique values to consider as categorical\n",
    "    'force_categorical': [],            # Columns to force as categorical\n",
    "    'force_continuous': [],             # Columns to force as continuous\n",
    "}\n",
    "\n",
    "# Configuration for ACS Income dataset\n",
    "# CONFIG = {\n",
    "#     'dataset_name': 'acs_income',  # Name for your dataset\n",
    "#     'short_name': 'inc',            # Short name for the dataset\n",
    "#     'output_dir': '/path/to/data',   # Where to save the H5 file (must match DATA_ROOT in run.sh)\n",
    "#     'data_source': 'openml',           # 'file' or 'openml'\n",
    "#     'file_path': None,                  # Path to your data file (if using 'file')\n",
    "#     'openml_id': 43137,                  # OpenML dataset ID (if using 'openml')\n",
    "#     'train_split': 0.70,\n",
    "#     'val_split': 0.15,\n",
    "#     'test_split': 0.15,\n",
    "#     'random_seed': 42,\n",
    "#     'categorical_threshold': 530,        # Max unique values to consider as categorical\n",
    "#     'force_categorical': [],            # Columns to force as categorical\n",
    "#     'force_continuous': [],             # Columns to force as continuous\n",
    "# }\n",
    "\n",
    "print(\"Configuration set up. Modify the CONFIG dictionary above as needed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1702c80d",
   "metadata": {},
   "source": [
    "## Step 1: Load Dataset\n",
    "\n",
    "Choose your data source and load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7570fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def load_dataset_from_file(file_path):\n",
    "    \"\"\"\n",
    "    Load dataset from various file formats.\n",
    "    \"\"\"\n",
    "    file_path = Path(file_path)\n",
    "    \n",
    "    if not file_path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    # Determine file format and load accordingly\n",
    "    if file_path.suffix.lower() == '.csv':\n",
    "        df = pd.read_csv(file_path)\n",
    "    elif file_path.suffix.lower() in ['.parquet', '.pq']:\n",
    "        df = pd.read_parquet(file_path)\n",
    "    elif file_path.suffix.lower() in ['.xlsx', '.xls']:\n",
    "        df = pd.read_excel(file_path)\n",
    "    elif file_path.suffix.lower() == '.json':\n",
    "        df = pd.read_json(file_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {file_path.suffix}\")\n",
    "    \n",
    "    print(f\"Loaded dataset from {file_path}\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def load_dataset_from_openml(dataset_id):\n",
    "    \"\"\"\n",
    "    Load dataset from OpenML.\n",
    "    \"\"\"\n",
    "    if not OPENML_AVAILABLE:\n",
    "        raise ImportError(\"OpenML is not available. Install with: pip install openml\")\n",
    "    \n",
    "    print(f\"Loading dataset {dataset_id} from OpenML...\")\n",
    "    dataset = openml.datasets.get_dataset(dataset_id)\n",
    "    X, y, categorical_indicator, attribute_names = dataset.get_data(\n",
    "        dataset_format=\"dataframe\", target=dataset.default_target_attribute\n",
    "    )\n",
    "    \n",
    "    # Combine features and target\n",
    "    df = X.copy()\n",
    "    if y is not None:\n",
    "        df[dataset.default_target_attribute] = y\n",
    "    \n",
    "    print(f\"Loaded OpenML dataset: {dataset.name}\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Default target: {dataset.default_target_attribute}\")\n",
    "    \n",
    "    return df, dataset.default_target_attribute, categorical_indicator\n",
    "\n",
    "# Load the dataset based on configuration\n",
    "df = None\n",
    "original_target = None\n",
    "openml_categorical_indicator = None\n",
    "\n",
    "if CONFIG['data_source'] == 'file':\n",
    "    if CONFIG['file_path'] is None:\n",
    "        print(\"Please set CONFIG['file_path'] to your data file path\")\n",
    "    else:\n",
    "        df = load_dataset_from_file(CONFIG['file_path'])\n",
    "        \n",
    "elif CONFIG['data_source'] == 'openml':\n",
    "    if CONFIG['openml_id'] is None:\n",
    "        print(\"Please set CONFIG['openml_id'] to the OpenML dataset ID\")\n",
    "    else:\n",
    "        df, original_target, openml_categorical_indicator = load_dataset_from_openml(CONFIG['openml_id'])\n",
    "        print(f\"Original target column: {original_target}\")\n",
    "\n",
    "if CONFIG['dataset_name'] == 'acs_income' and df is not None:\n",
    "    # Binarize the income column\n",
    "    if 'PINCP' in df.columns:\n",
    "        df['PINCP'] = (df['PINCP'] > 50000).astype(int)\n",
    "        print(\"\\nBinarized 'PINCP' column to indicate income > 50K\")\n",
    "        \n",
    "if df is not None:\n",
    "    print(\"\\nDataset loaded successfully!\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    print(f\"Data types:\\n{df.dtypes}\")\n",
    "else:\n",
    "    print(\"Please configure the data source properly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ea0989",
   "metadata": {},
   "source": [
    "## Step 2: Explore Dataset\n",
    "\n",
    "Get familiar with your dataset structure and identify potential issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e2edc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    print(\"=== Dataset Overview ===\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    print(\"\\n=== Missing Values ===\")\n",
    "    missing_info = df.isnull().sum()\n",
    "    missing_info = missing_info[missing_info > 0].sort_values(ascending=False)\n",
    "    if len(missing_info) > 0:\n",
    "        print(missing_info)\n",
    "    else:\n",
    "        print(\"No missing values found!\")\n",
    "    \n",
    "    print(\"\\n=== Data Types ===\")\n",
    "    print(df.dtypes.value_counts())\n",
    "    \n",
    "    print(\"\\n=== Unique Values per Column ===\")\n",
    "    unique_counts = df.nunique().sort_values(ascending=False)\n",
    "    print(unique_counts)\n",
    "    \n",
    "    print(\"\\n=== Sample Data ===\")\n",
    "    display(df.head())\n",
    "    \n",
    "    print(\"\\n=== Statistical Summary ===\")\n",
    "    display(df.describe(include='all'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8f37ca",
   "metadata": {},
   "source": [
    "## Step 3: Configure Target Variables\n",
    "\n",
    "Select primary target and any additional targets you want to include."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5640d412",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    print(\"Available columns for target selection:\")\n",
    "    for i, col in enumerate(df.columns):\n",
    "        print(f\"{i:2d}: {col} (dtype: {df[col].dtype}, unique: {df[col].nunique()})\")\n",
    "    \n",
    "    print(\"\\nConfigure your targets below:\")\n",
    "\n",
    "# Configure Higgs targets here\n",
    "TARGET_CONFIG = {\n",
    "    'primary_target': 'Target',      # e.g., 'target' or column index\n",
    "    'additional_targets': ['m_jj', 'm_jjj', 'm_lv', 'm_jlv', 'm_bb', 'm_wbb', 'm_wwbb'],\n",
    "    'target_types': {'Target': 'binary', 'm_jj': 'regression', 'm_jjj': 'regression', 'm_lv': 'regression', 'm_jlv': 'regression', 'm_bb': 'regression', 'm_wbb': 'regression', 'm_wwbb': 'regression'}  # e.g., {'target': 'binary', 'income': 'classification'}\n",
    "}\n",
    "\n",
    "# Configure ACS Income targets here\n",
    "# TARGET_CONFIG = {\n",
    "#     'primary_target': 'PINCP',\n",
    "#     'additional_targets': ['MAR'],\n",
    "#     'target_types': {\n",
    "#         'PINCP': 'binary',\n",
    "#         'MAR': 'classification'\n",
    "#     }\n",
    "# }\n",
    "\n",
    "print(\"\\nPlease configure TARGET_CONFIG above with your target columns.\")\n",
    "print(\"Target types can be: 'binary', 'classification', 'regression'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b353e703",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_targets(df, target_config):\n",
    "    \"\"\"\n",
    "    Process and validate target configuration.\n",
    "    \"\"\"\n",
    "    if target_config['primary_target'] is None:\n",
    "        raise ValueError(\"Please specify a primary target\")\n",
    "    \n",
    "    # Convert column indices to names if necessary\n",
    "    def get_column_name(col_ref):\n",
    "        if isinstance(col_ref, int):\n",
    "            return df.columns[col_ref]\n",
    "        return col_ref\n",
    "    \n",
    "    # Get all target columns\n",
    "    primary_target = get_column_name(target_config['primary_target'])\n",
    "    additional_targets = [get_column_name(col) for col in target_config['additional_targets']]\n",
    "    \n",
    "    all_targets = [primary_target] + additional_targets\n",
    "    \n",
    "    # Validate targets exist\n",
    "    for target in all_targets:\n",
    "        if target not in df.columns:\n",
    "            raise ValueError(f\"Target column '{target}' not found in dataset\")\n",
    "    \n",
    "    # Analyze targets\n",
    "    target_info = {}\n",
    "    for target in all_targets:\n",
    "        unique_vals = df[target].nunique()\n",
    "        null_count = df[target].isnull().sum()\n",
    "        \n",
    "        # Auto-detect target type if not specified\n",
    "        if target not in target_config['target_types']:\n",
    "            if unique_vals == 2:\n",
    "                suggested_type = 'binary'\n",
    "            elif unique_vals <= 20 and df[target].dtype in ['object', 'category']:\n",
    "                suggested_type = 'classification'\n",
    "            elif unique_vals <= 20:\n",
    "                suggested_type = 'classification'\n",
    "            else:\n",
    "                suggested_type = 'regression'\n",
    "        else:\n",
    "            suggested_type = target_config['target_types'][target]\n",
    "        \n",
    "        target_info[target] = {\n",
    "            'unique_values': unique_vals,\n",
    "            'null_count': null_count,\n",
    "            'dtype': str(df[target].dtype),\n",
    "            'type': suggested_type,\n",
    "            'sample_values': df[target].dropna().unique()[:10].tolist()\n",
    "        }\n",
    "    \n",
    "    return all_targets, target_info\n",
    "\n",
    "# Process targets\n",
    "if df is not None and TARGET_CONFIG['primary_target'] is not None:\n",
    "    try:\n",
    "        target_columns, target_info = process_targets(df, TARGET_CONFIG)\n",
    "        \n",
    "        print(\"=== Target Analysis ===\")\n",
    "        for target, info in target_info.items():\n",
    "            print(f\"\\nTarget: {target}\")\n",
    "            print(f\"  Type: {info['type']}\")\n",
    "            print(f\"  Unique values: {info['unique_values']}\")\n",
    "            print(f\"  Null count: {info['null_count']}\")\n",
    "            print(f\"  Data type: {info['dtype']}\")\n",
    "            print(f\"  Sample values: {info['sample_values']}\")\n",
    "            \n",
    "            # Show value distribution for categorical targets\n",
    "            if info['type'] in ['binary', 'classification'] and info['unique_values'] <= 20:\n",
    "                print(f\"  Value distribution:\")\n",
    "                value_counts = df[target].value_counts()\n",
    "                for val, count in value_counts.head(10).items():\n",
    "                    print(f\"    {val}: {count} ({count/len(df)*100:.1f}%)\")\n",
    "        \n",
    "        print(f\"\\nTotal targets configured: {len(target_columns)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing targets: {e}\")\n",
    "        target_columns = None\n",
    "else:\n",
    "    print(\"Please configure targets first\")\n",
    "    target_columns = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36c014f",
   "metadata": {},
   "source": [
    "## Step 4: Feature Processing\n",
    "\n",
    "Separate features into categorical and continuous, handle missing values, and encode appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f953f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_features(df, target_columns, config):\n",
    "    \"\"\"\n",
    "    Analyze features and categorize them as categorical or continuous.\n",
    "    \"\"\"\n",
    "    feature_columns = [col for col in df.columns if col not in target_columns]\n",
    "    \n",
    "    categorical_cols = []\n",
    "    continuous_cols = []\n",
    "    \n",
    "    print(\"=== Feature Analysis ===\")\n",
    "    print(f\"Total features: {len(feature_columns)}\")\n",
    "    \n",
    "    for col in feature_columns:\n",
    "        unique_vals = df[col].nunique()\n",
    "        dtype = df[col].dtype\n",
    "        null_count = df[col].isnull().sum()\n",
    "        \n",
    "        # Determine if categorical or continuous\n",
    "        if col in config['force_categorical']:\n",
    "            feature_type = 'categorical'\n",
    "        elif col in config['force_continuous']:\n",
    "            feature_type = 'continuous'\n",
    "        elif dtype in ['object', 'category', 'bool']:\n",
    "            feature_type = 'categorical'\n",
    "        elif unique_vals <= config['categorical_threshold']:\n",
    "            feature_type = 'categorical'\n",
    "        else:\n",
    "            feature_type = 'continuous'\n",
    "        \n",
    "        if feature_type == 'categorical':\n",
    "            categorical_cols.append(col)\n",
    "        else:\n",
    "            continuous_cols.append(col)\n",
    "        \n",
    "        print(f\"{col:30s} | {feature_type:12s} | unique: {unique_vals:6d} | nulls: {null_count:6d} | dtype: {dtype}\")\n",
    "    \n",
    "    print(f\"\\nCategorical features: {len(categorical_cols)}\")\n",
    "    print(f\"Continuous features: {len(continuous_cols)}\")\n",
    "    \n",
    "    return categorical_cols, continuous_cols\n",
    "\n",
    "if df is not None and target_columns is not None:\n",
    "    categorical_features, continuous_features = analyze_features(df, target_columns, CONFIG)\n",
    "else:\n",
    "    print(\"Please complete previous steps first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6d59a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_features(df, categorical_cols, continuous_cols, target_columns):\n",
    "    \"\"\"\n",
    "    Preprocess features: handle missing values, encode categorical variables, scale continuous variables.\n",
    "    \"\"\"\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    print(\"=== Preprocessing Features ===\")\n",
    "    \n",
    "    # Handle missing values\n",
    "    print(\"\\nHandling missing values...\")\n",
    "    \n",
    "    # For categorical features: fill with 'Unknown'\n",
    "    for col in categorical_cols:\n",
    "        if df_processed[col].isnull().sum() > 0:\n",
    "            df_processed[col] = df_processed[col].fillna('Unknown')\n",
    "            print(f\"  {col}: filled {df[col].isnull().sum()} missing values with 'Unknown'\")\n",
    "    \n",
    "    # For continuous features: fill with median\n",
    "    for col in continuous_cols:\n",
    "        if df_processed[col].isnull().sum() > 0:\n",
    "            median_val = df_processed[col].median()\n",
    "            df_processed[col] = df_processed[col].fillna(median_val)\n",
    "            print(f\"  {col}: filled {df[col].isnull().sum()} missing values with median ({median_val})\")\n",
    "    \n",
    "    # Encode categorical features\n",
    "    print(\"\\nEncoding categorical features...\")\n",
    "    label_encoders = {}\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        le = LabelEncoder()\n",
    "        df_processed[col] = le.fit_transform(df_processed[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "        print(f\"  {col}: encoded {len(le.classes_)} unique values\")\n",
    "\n",
    "    # Scale continuous features\n",
    "    print(\"\\nScaling continuous features...\")\n",
    "    scaler = StandardScaler()\n",
    "    for col in continuous_cols:\n",
    "        df_processed[col] = scaler.fit_transform(df_processed[[col]])\n",
    "        print(f\"  {col}: scaled with StandardScaler\")\n",
    "    \n",
    "    # Handle target encoding\n",
    "    print(\"\\nProcessing targets...\")\n",
    "    target_encoders = {}\n",
    "    \n",
    "    for target in target_columns:\n",
    "        target_type = target_info[target]['type']\n",
    "        \n",
    "        if target_type in ['binary', 'classification']:\n",
    "            # Handle missing values in targets\n",
    "            if df_processed[target].isnull().sum() > 0:\n",
    "                print(f\"  Warning: {target} has {df_processed[target].isnull().sum()} missing values\")\n",
    "                # Drop rows with missing targets\n",
    "                df_processed = df_processed.dropna(subset=[target])\n",
    "                print(f\"  Dropped rows with missing {target} values\")\n",
    "            \n",
    "            le = LabelEncoder()\n",
    "            df_processed[target] = le.fit_transform(df_processed[target].astype(str))\n",
    "            target_encoders[target] = le\n",
    "            print(f\"  {target}: encoded as {target_type} with {len(le.classes_)} classes\")\n",
    "        \n",
    "        elif target_type == 'regression':\n",
    "            # Ensure numeric type\n",
    "            df_processed[target] = pd.to_numeric(df_processed[target], errors='coerce')\n",
    "            # Handle missing values\n",
    "            if df_processed[target].isnull().sum() > 0:\n",
    "                print(f\"  Warning: {target} has {df_processed[target].isnull().sum()} missing values after conversion\")\n",
    "                df_processed = df_processed.dropna(subset=[target])\n",
    "            print(f\"  {target}: processed as regression target\")\n",
    "    \n",
    "    print(f\"\\nFinal dataset shape: {df_processed.shape}\")\n",
    "    \n",
    "    return df_processed, label_encoders, target_encoders\n",
    "\n",
    "if df is not None and target_columns is not None:\n",
    "    try:\n",
    "        df_processed, feature_encoders, target_encoders = preprocess_features(\n",
    "            df, categorical_features, continuous_features, target_columns\n",
    "        )\n",
    "        print(\"\\nPreprocessing completed successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during preprocessing: {e}\")\n",
    "        df_processed = None\n",
    "else:\n",
    "    print(\"Please complete previous steps first\")\n",
    "    df_processed = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08de37ca",
   "metadata": {},
   "source": [
    "## Step 5: Create H5 Dataset\n",
    "\n",
    "Save the processed dataset in H5 format compatible with our modular dataset system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56ba478",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_config(dataset_name, short_name, target_columns, target_info, categorical_features, continuous_features):\n",
    "    \"\"\"\n",
    "    Create a dataset configuration dictionary.\n",
    "    \"\"\"\n",
    "    # Create task type mapping\n",
    "    task_types = {}\n",
    "    task_out_dims = {}\n",
    "    \n",
    "    for target in target_columns:\n",
    "        task_type = target_info[target]['type']\n",
    "        task_types[target] = task_type\n",
    "        \n",
    "        if task_type in ['binary', 'regression']:\n",
    "            task_out_dims[target] = 1\n",
    "        elif task_type == 'classification':\n",
    "            task_out_dims[target] = target_info[target]['unique_values']\n",
    "    \n",
    "    config = {\n",
    "        'data': {\n",
    "            'name': dataset_name,\n",
    "            'short_name': short_name,\n",
    "            'format': 'h5',\n",
    "            'path': f'/{dataset_name}/',\n",
    "            'tasks': target_columns,\n",
    "            'task_type': task_types,\n",
    "            'task_out_dim': task_out_dims,\n",
    "            'num_features': len(categorical_features) + len(continuous_features),\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return config\n",
    "\n",
    "if df_processed is not None:\n",
    "    # Create output directory\n",
    "    output_dir = Path(f\"{CONFIG['output_dir']}/{CONFIG['dataset_name']}\")\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Create H5 file path\n",
    "    h5_filename = \"train_val_test.h5\"\n",
    "    h5_path = output_dir / h5_filename\n",
    "    \n",
    "    print(f\"Creating H5 dataset: {h5_path}\")\n",
    "    \n",
    "    try:      \n",
    "        # Create configuration\n",
    "        dataset_config = create_dataset_config(\n",
    "            CONFIG['dataset_name'], CONFIG['short_name'], target_columns, target_info, \n",
    "            categorical_features, continuous_features\n",
    "        )\n",
    "        \n",
    "        # Save configuration\n",
    "        config_path = output_dir / \"config.yaml\"\n",
    "        \n",
    "        import yaml\n",
    "        with open(config_path, 'w') as f:\n",
    "            yaml.dump(dataset_config, f, default_flow_style=False)\n",
    "        \n",
    "        print(f\"Configuration saved: {config_path}\")\n",
    "\n",
    "        # Create H5 dataset\n",
    "        create_h5_from_dataframe(\n",
    "            df=df_processed,\n",
    "            output_path=str(h5_path),\n",
    "            tasks=target_columns,\n",
    "            task_types=dataset_config['data']['task_type'],\n",
    "            categorical_cols=categorical_features,\n",
    "            continuous_cols=continuous_features,\n",
    "            train_split=CONFIG['train_split'],\n",
    "            val_split=CONFIG['val_split'],\n",
    "            test_split=CONFIG['test_split'],\n",
    "            random_seed=CONFIG['random_seed'],\n",
    "            use_stratified=True\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nH5 dataset created successfully: {h5_path}\")\n",
    "        \n",
    "        h5_created = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating H5 dataset: {e}\")\n",
    "        h5_created = False\n",
    "        \n",
    "else:\n",
    "    print(\"Please complete preprocessing first\")\n",
    "    h5_created = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8a4b87",
   "metadata": {},
   "source": [
    "## Step 6: Validation\n",
    "\n",
    "Validate the created H5 dataset to ensure it's properly formatted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256ee2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if h5_created:\n",
    "    print(\"=== H5 Dataset Validation ===\")\n",
    "    \n",
    "    # Inspect the H5 structure\n",
    "    print(\"\\n--- H5 File Structure ---\")\n",
    "    inspect_h5_dataset(str(h5_path))\n",
    "    \n",
    "    # Validate the dataset\n",
    "    print(\"\\n--- Validation Results ---\")\n",
    "    validation_result = validate_h5_dataset(str(h5_path), target_columns)\n",
    "    \n",
    "    print(f\"Valid: {validation_result['valid']}\")\n",
    "    print(f\"Splits found: {validation_result['splits']}\")\n",
    "    print(f\"Tasks found: {validation_result['tasks_found']}\")\n",
    "    \n",
    "    if validation_result['errors']:\n",
    "        print(\"\\nErrors found:\")\n",
    "        for error in validation_result['errors']:\n",
    "            print(f\"  - {error}\")\n",
    "    \n",
    "    print(\"\\n--- Sample Counts ---\")\n",
    "    for split, count in validation_result['num_samples'].items():\n",
    "        print(f\"  {split}: {count:,} samples\")\n",
    "    \n",
    "    print(\"\\n--- Feature Shapes ---\")\n",
    "    for key, shape in validation_result['feature_shapes'].items():\n",
    "        print(f\"  {key}: {shape}\")\n",
    "    \n",
    "    # Test loading with our dataset class\n",
    "    print(\"\\n--- Testing Dataset Loading ---\")\n",
    "    try:\n",
    "        from data.dataset import H5Dataset\n",
    "        \n",
    "        # Test loading train split\n",
    "        test_dataset = H5Dataset(\n",
    "            h5_path=str(h5_path),\n",
    "            split='train',\n",
    "            tasks=target_columns,\n",
    "            task_types={target: target_info[target]['type'] for target in target_columns},\n",
    "            seperate_ft_types=True\n",
    "        )\n",
    "        \n",
    "        print(f\"Dataset loaded successfully!\")\n",
    "        print(f\"Length: {len(test_dataset)}\")\n",
    "        print(f\"Field dimensions: {test_dataset.field_dims}\")\n",
    "        \n",
    "        # Test getting a sample\n",
    "        sample = test_dataset[0]\n",
    "        print(f\"Sample keys: {list(sample.keys())}\")\n",
    "        print(f\"Feature keys: {list(sample['features'].keys()) if isinstance(sample['features'], dict) else 'tensor'}\")\n",
    "        print(f\"Target keys: {list(sample['targets'].keys())}\")\n",
    "        \n",
    "        if isinstance(sample['features'], dict):\n",
    "            print(f\"Categorical features shape: {sample['features']['categorical'].shape}\")\n",
    "            print(f\"Continuous features shape: {sample['features']['continuous'].shape}\")\n",
    "        else:\n",
    "            print(f\"Combined features shape: {sample['features'].shape}\")\n",
    "        \n",
    "        print(\"\\nAll validation tests passed! âœ…\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"No H5 dataset to validate\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mtt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
